{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kkqJHOoweMg",
        "outputId": "7c48c182-0661-441c-93a3-fde33c026058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Metric    Cosine       Dot  Euclidean      Manh\n",
            "0      Prec@5  0.514667  0.496000   0.234667  0.418667\n",
            "1    Recall@5  0.281228  0.272390   0.125755  0.218421\n",
            "2        F1@5  0.353943  0.341985   0.160339  0.284768\n",
            "3    Avg Prec  0.238683  0.233784   0.098797  0.184127\n",
            "4  Recip Rank  0.801111  0.787778   0.418222  0.738889\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import ast\n",
        "import math\n",
        "\n",
        "def extract_alias_from_textbook_name(name: str) -> str | None:\n",
        "    if not isinstance(name, str):\n",
        "        return None\n",
        "    m = re.search(r'\\(\"([^\"]+)\"\\)', name)\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "def parse_ground_truth_ranges(gt_str: str) -> list[tuple[int,int]]:\n",
        "    if not isinstance(gt_str, str):\n",
        "        return []\n",
        "    # pull out all [a,b] pairs\n",
        "    pairs = re.findall(r'\\[\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\]', gt_str)\n",
        "    return [(int(a), int(b)) for a, b in pairs]\n",
        "\n",
        "def range_to_pages(ranges: list[tuple[int,int]]) -> set[int]:\n",
        "    \"\"\"\n",
        "    Expand inclusive ranges to a set of page numbers.\n",
        "    e.g. [(89,91), (97,99)] -> {89,90,91,97,98,99}\n",
        "    \"\"\"\n",
        "    pages = set()\n",
        "    for a, b in ranges:\n",
        "        if a <= b:\n",
        "            pages.update(range(a, b+1))\n",
        "        else:\n",
        "            pages.update(range(b, a+1))\n",
        "    return pages\n",
        "\n",
        "def parse_top5_string(s: str) -> list[tuple[str,int]]:\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    # Grab the inside of parentheses following t5 =\n",
        "    m = re.search(r't5\\s*=\\s*\\((.*)\\)\\s*$', s.strip())\n",
        "    inside = m.group(1) if m else s.strip()\n",
        "\n",
        "    # Find groups like alias-(numbers)\n",
        "    groups = re.findall(r'([A-Za-z0-9_]+)\\s*-\\s*\\(\\s*([0-9,\\s]+)\\s*\\)', inside)\n",
        "    results = []\n",
        "    for alias, nums in groups:\n",
        "        pages = [int(x) for x in re.findall(r'\\d+', nums)]\n",
        "        for p in pages:\n",
        "            results.append((alias.strip(), p))\n",
        "    return results[:5]  # ensure top-5 only\n",
        "\n",
        "def metrics_at_k(retrieved_pairs: list[tuple[str,int]], gt_alias: str, gt_pages: set[int], k: int = 5):\n",
        "    topk = retrieved_pairs[:k]\n",
        "    rel = []\n",
        "    for alias, page in topk:\n",
        "        rel.append(1 if (alias == gt_alias and page in gt_pages) else 0)\n",
        "\n",
        "    # Precision@k\n",
        "    prec = sum(rel) / max(k, 1)\n",
        "\n",
        "    # Recall@k (denominator = total number of relevant pages)\n",
        "    denom = max(len(gt_pages), 1)\n",
        "    rec = sum(rel) / denom\n",
        "\n",
        "    # F1@k\n",
        "    f1 = 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
        "\n",
        "    # Average Precision (AP)\n",
        "    running_hits = 0\n",
        "    ap_sum = 0.0\n",
        "    for i, r in enumerate(rel, start=1):\n",
        "        if r == 1:\n",
        "            running_hits += 1\n",
        "            ap_sum += running_hits / i\n",
        "    ap = ap_sum / denom  # standard AP normalization by total relevant\n",
        "\n",
        "    # Reciprocal Rank (RR)\n",
        "    rr = 0.0\n",
        "    for i, r in enumerate(rel, start=1):\n",
        "        if r == 1:\n",
        "            rr = 1.0 / i\n",
        "            break\n",
        "\n",
        "    return dict(Prec5=prec, Recall5=rec, F15=f1, AvgPrec=ap, RR=rr)\n",
        "\n",
        "EXCEL_PATH = \"Vector Discovery Benchmark 75.xlsx\"\n",
        "\n",
        "df = pd.read_excel(EXCEL_PATH, sheet_name=\"Sheet1\")\n",
        "\n",
        "COL_TEXTBOOK = \"Textbook Name\"\n",
        "COL_GT = \"Ground Truth: [] means all pages in sequence\"\n",
        "COL_COS = \"Cosine Distance top 5\"\n",
        "COL_DOT = \"Dot product top 5\"\n",
        "COL_EUC = \"Euclidean distance top 5\"\n",
        "COL_MAN = \"Manhattan distance top 5\"\n",
        "\n",
        "# List to store the results for each query\n",
        "rows = []\n",
        "for _, row in df.iterrows():\n",
        "    gt_alias = extract_alias_from_textbook_name(row.get(COL_TEXTBOOK, \"\"))\n",
        "    gt_ranges = parse_ground_truth_ranges(row.get(COL_GT, \"\"))\n",
        "    gt_pages = range_to_pages(gt_ranges)\n",
        "\n",
        "    # Parse each similarity's top-5 list as (alias, page) pairs\n",
        "    cos_pairs = parse_top5_string(str(row.get(COL_COS, \"\")))\n",
        "    dot_pairs = parse_top5_string(str(row.get(COL_DOT, \"\")))\n",
        "    euc_pairs = parse_top5_string(str(row.get(COL_EUC, \"\")))\n",
        "    man_pairs = parse_top5_string(str(row.get(COL_MAN, \"\")))\n",
        "\n",
        "    # Calculate the IR metrics for each similarity\n",
        "    cos_m = metrics_at_k(cos_pairs, gt_alias, gt_pages, k=5)\n",
        "    dot_m = metrics_at_k(dot_pairs, gt_alias, gt_pages, k=5)\n",
        "    euc_m = metrics_at_k(euc_pairs, gt_alias, gt_pages, k=5)\n",
        "    man_m = metrics_at_k(man_pairs, gt_alias, gt_pages, k=5)\n",
        "\n",
        "    # Append the results for each query to the rows list\n",
        "    rows.append({\n",
        "        \"Query ID\": row.get(\"Query ID\"),\n",
        "        \"Cosine_Prec@5\": cos_m[\"Prec5\"],   \"Cosine_Recall@5\": cos_m[\"Recall5\"], \"Cosine_F1@5\": cos_m[\"F15\"],\n",
        "        \"Cosine_AvgPrec\": cos_m[\"AvgPrec\"], \"Cosine_RR\": cos_m[\"RR\"],\n",
        "\n",
        "        \"Dot_Prec@5\": dot_m[\"Prec5\"],      \"Dot_Recall@5\": dot_m[\"Recall5\"],    \"Dot_F1@5\": dot_m[\"F15\"],\n",
        "        \"Dot_AvgPrec\": dot_m[\"AvgPrec\"],   \"Dot_RR\": dot_m[\"RR\"],\n",
        "\n",
        "        \"Euclidean_Prec@5\": euc_m[\"Prec5\"],   \"Euclidean_Recall@5\": euc_m[\"Recall5\"], \"Euclidean_F1@5\": euc_m[\"F15\"],\n",
        "        \"Euclidean_AvgPrec\": euc_m[\"AvgPrec\"], \"Euclidean_RR\": euc_m[\"RR\"],\n",
        "\n",
        "        \"Manhattan_Prec@5\": man_m[\"Prec5\"],   \"Manhattan_Recall@5\": man_m[\"Recall5\"], \"Manhattan_F1@5\": man_m[\"F15\"],\n",
        "        \"Manhattan_AvgPrec\": man_m[\"AvgPrec\"], \"Manhattan_RR\": man_m[\"RR\"],\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the rows list\n",
        "per_query = pd.DataFrame(rows)\n",
        "\n",
        "# Calculate averages over all queries\n",
        "avg_row = {\"Query ID\": \"AVERAGE\"}\n",
        "for col in per_query.columns:\n",
        "    if col == \"Query ID\":\n",
        "        continue\n",
        "    avg_row[col] = per_query[col].mean()\n",
        "\n",
        "# Append the average row to the DataFrame\n",
        "summary = pd.concat([per_query, pd.DataFrame([avg_row])], ignore_index=True)\n",
        "\n",
        "# Create a final summary table\n",
        "final_summary = pd.DataFrame({\n",
        "    \"Metric\": [\"Prec@5\", \"Recall@5\", \"F1@5\", \"Avg Prec\", \"Recip Rank\"],\n",
        "    \"Cosine\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_RR\"].item(),\n",
        "    ],\n",
        "    \"Dot\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_RR\"].item(),\n",
        "    ],\n",
        "    \"Euclidean\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_RR\"].item(),\n",
        "    ],\n",
        "    \"Manh\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_RR\"].item(),\n",
        "    ],\n",
        "})\n",
        "\n",
        "# Print the final values\n",
        "print(final_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multi page queries\n",
        "import pandas as pd\n",
        "import re\n",
        "import ast\n",
        "import math\n",
        "\n",
        "\n",
        "def extract_alias_from_textbook_name(name: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Textbook Name looks like:  Artificial Intelligence a Modern Approach (\"aima\")\n",
        "    Return the alias inside the quotes -> 'aima'\n",
        "    \"\"\"\n",
        "    if not isinstance(name, str):\n",
        "        return None\n",
        "    m = re.search(r'\\(\"([^\"]+)\"\\)', name)\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "def parse_ground_truth_ranges(gt_str: str) -> list[tuple[int,int]]:\n",
        "\n",
        "    if not isinstance(gt_str, str):\n",
        "        return []\n",
        "    # pull out all [a,b] pairs\n",
        "    pairs = re.findall(r'\\[\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\]', gt_str)\n",
        "    return [(int(a), int(b)) for a, b in pairs]\n",
        "\n",
        "def range_to_pages(ranges: list[tuple[int,int]]) -> set[int]:\n",
        "\n",
        "    pages = set()\n",
        "    for a, b in ranges:\n",
        "        if a <= b:\n",
        "            pages.update(range(a, b+1))\n",
        "        else:\n",
        "            pages.update(range(b, a+1))\n",
        "    return pages\n",
        "\n",
        "def parse_top5_string(s: str) -> list[tuple[str,int]]:\n",
        "\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    m = re.search(r't5\\s*=\\s*\\((.*)\\)\\s*$', s.strip())\n",
        "    inside = m.group(1) if m else s.strip()\n",
        "\n",
        "    groups = re.findall(r'([A-Za-z0-9_]+)\\s*-\\s*\\(\\s*([0-9,\\s]+)\\s*\\)', inside)\n",
        "    results = []\n",
        "    for alias, nums in groups:\n",
        "        pages = [int(x) for x in re.findall(r'\\d+', nums)]\n",
        "        for p in pages:\n",
        "            results.append((alias.strip(), p))\n",
        "    return results[:5]\n",
        "\n",
        "\n",
        "def metrics_at_k(retrieved_pairs: list[tuple[str,int]], gt_alias: str, gt_pages: set[int], k: int = 5):\n",
        "\n",
        "    topk = retrieved_pairs[:k]\n",
        "    rel = []\n",
        "    for alias, page in topk:\n",
        "        rel.append(1 if (alias == gt_alias and page in gt_pages) else 0)\n",
        "\n",
        "    # Precision@k\n",
        "    prec = sum(rel) / max(k, 1)\n",
        "\n",
        "    # Recall@k (denominator = total number of relevant pages)\n",
        "    denom = max(len(gt_pages), 1)\n",
        "    rec = sum(rel) / denom\n",
        "\n",
        "    # F1@k\n",
        "    f1 = 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
        "\n",
        "    # Average Precision (AP)\n",
        "    running_hits = 0\n",
        "    ap_sum = 0.0\n",
        "    for i, r in enumerate(rel, start=1):\n",
        "        if r == 1:\n",
        "            running_hits += 1\n",
        "            ap_sum += running_hits / i\n",
        "    ap = ap_sum / denom  # standard AP normalization by total relevant\n",
        "\n",
        "    # Reciprocal Rank (RR)\n",
        "    rr = 0.0\n",
        "    for i, r in enumerate(rel, start=1):\n",
        "        if r == 1:\n",
        "            rr = 1.0 / i\n",
        "            break\n",
        "\n",
        "    return dict(Prec5=prec, Recall5=rec, F15=f1, AvgPrec=ap, RR=rr)\n",
        "\n",
        "\n",
        "EXCEL_PATH = \"Vector Discovery Benchmark 75.xlsx\"\n",
        "df = pd.read_excel(EXCEL_PATH, sheet_name=\"Sheet1\")\n",
        "# Define the column names in the dataset\n",
        "COL_TEXTBOOK = \"Textbook Name\"\n",
        "COL_GT = \"Ground Truth: [] means all pages in sequence\"\n",
        "COL_COS = \"Cosine Distance top 5\"\n",
        "COL_DOT = \"Dot product top 5\"\n",
        "COL_EUC = \"Euclidean distance top 5\"\n",
        "COL_MAN = \"Manhattan distance top 5\"\n",
        "\n",
        "df_filtered = df[df[\"Query ID\"].str.startswith(\"MP-\")]\n",
        "\n",
        "rows = []\n",
        "for _, row in df_filtered.iterrows():\n",
        "    gt_alias = extract_alias_from_textbook_name(row.get(COL_TEXTBOOK, \"\"))\n",
        "    gt_ranges = parse_ground_truth_ranges(row.get(COL_GT, \"\"))\n",
        "    gt_pages = range_to_pages(gt_ranges)\n",
        "\n",
        "    cos_pairs = parse_top5_string(str(row.get(COL_COS, \"\")))\n",
        "    dot_pairs = parse_top5_string(str(row.get(COL_DOT, \"\")))\n",
        "    euc_pairs = parse_top5_string(str(row.get(COL_EUC, \"\")))\n",
        "    man_pairs = parse_top5_string(str(row.get(COL_MAN, \"\")))\n",
        "\n",
        "    cos_m = metrics_at_k(cos_pairs, gt_alias, gt_pages, k=5)\n",
        "    dot_m = metrics_at_k(dot_pairs, gt_alias, gt_pages, k=5)\n",
        "    euc_m = metrics_at_k(euc_pairs, gt_alias, gt_pages, k=5)\n",
        "    man_m = metrics_at_k(man_pairs, gt_alias, gt_pages, k=5)\n",
        "\n",
        "    rows.append({\n",
        "        \"Query ID\": row.get(\"Query ID\"),\n",
        "        \"Cosine_Prec@5\": cos_m[\"Prec5\"],   \"Cosine_Recall@5\": cos_m[\"Recall5\"], \"Cosine_F1@5\": cos_m[\"F15\"],\n",
        "        \"Cosine_AvgPrec\": cos_m[\"AvgPrec\"], \"Cosine_RR\": cos_m[\"RR\"],\n",
        "\n",
        "        \"Dot_Prec@5\": dot_m[\"Prec5\"],      \"Dot_Recall@5\": dot_m[\"Recall5\"],    \"Dot_F1@5\": dot_m[\"F15\"],\n",
        "        \"Dot_AvgPrec\": dot_m[\"AvgPrec\"],   \"Dot_RR\": dot_m[\"RR\"],\n",
        "\n",
        "        \"Euclidean_Prec@5\": euc_m[\"Prec5\"],   \"Euclidean_Recall@5\": euc_m[\"Recall5\"], \"Euclidean_F1@5\": euc_m[\"F15\"],\n",
        "        \"Euclidean_AvgPrec\": euc_m[\"AvgPrec\"], \"Euclidean_RR\": euc_m[\"RR\"],\n",
        "\n",
        "        \"Manhattan_Prec@5\": man_m[\"Prec5\"],   \"Manhattan_Recall@5\": man_m[\"Recall5\"], \"Manhattan_F1@5\": man_m[\"F15\"],\n",
        "        \"Manhattan_AvgPrec\": man_m[\"AvgPrec\"], \"Manhattan_RR\": man_m[\"RR\"],\n",
        "    })\n",
        "\n",
        "per_query = pd.DataFrame(rows)\n",
        "\n",
        "avg_row = {\"Query ID\": \"AVERAGE\"}\n",
        "for col in per_query.columns:\n",
        "    if col == \"Query ID\":\n",
        "        continue\n",
        "    avg_row[col] = per_query[col].mean()\n",
        "\n",
        "# Append the average row to the DataFrame\n",
        "summary = pd.concat([per_query, pd.DataFrame([avg_row])], ignore_index=True)\n",
        "\n",
        "# Create a final summary table\n",
        "final_summary_multipage = pd.DataFrame({\n",
        "    \"Metric\": [\"Prec@5\", \"Recall@5\", \"F1@5\", \"Avg Prec\", \"Recip Rank\"],\n",
        "    \"Cosine\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_RR\"].item(),\n",
        "    ],\n",
        "    \"Dot\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_RR\"].item(),\n",
        "    ],\n",
        "    \"Euclidean\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_RR\"].item(),\n",
        "    ],\n",
        "    \"Manh\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_RR\"].item(),\n",
        "    ],\n",
        "})\n",
        "\n",
        "# Print the final values\n",
        "print(final_summary_multipage)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiGmm91r0Ukl",
        "outputId": "62c6e00a-acd2-42c2-eb6a-fa307620f98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Metric    Cosine       Dot  Euclidean      Manh\n",
            "0      Prec@5  0.457143  0.442857   0.157143  0.342857\n",
            "1    Recall@5  0.239042  0.231106   0.069706  0.176786\n",
            "2        F1@5  0.306394  0.296190   0.093250  0.232598\n",
            "3    Avg Prec  0.176450  0.169156   0.037479  0.138313\n",
            "4  Recip Rank  0.690476  0.654762   0.246429  0.571429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#conceptual queries\n",
        "import pandas as pd\n",
        "import re\n",
        "import ast\n",
        "import math\n",
        "\n",
        "\n",
        "def extract_alias_from_textbook_name(name: str) -> str | None:\n",
        "\n",
        "    if not isinstance(name, str):\n",
        "        return None\n",
        "    m = re.search(r'\\(\"([^\"]+)\"\\)', name)\n",
        "    return m.group(1).strip() if m else None\n",
        "\n",
        "def parse_ground_truth_ranges(gt_str: str) -> list[tuple[int,int]]:\n",
        "\n",
        "    if not isinstance(gt_str, str):\n",
        "        return []\n",
        "    # pull out all [a,b] pairs\n",
        "    pairs = re.findall(r'\\[\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\]', gt_str)\n",
        "    return [(int(a), int(b)) for a, b in pairs]\n",
        "\n",
        "def range_to_pages(ranges: list[tuple[int,int]]) -> set[int]:\n",
        "\n",
        "    pages = set()\n",
        "    for a, b in ranges:\n",
        "        if a <= b:\n",
        "            pages.update(range(a, b+1))\n",
        "        else:\n",
        "            pages.update(range(b, a+1))\n",
        "    return pages\n",
        "\n",
        "def parse_top5_string(s: str) -> list[tuple[str,int]]:\n",
        "\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    m = re.search(r't5\\s*=\\s*\\((.*)\\)\\s*$', s.strip())\n",
        "    inside = m.group(1) if m else s.strip()\n",
        "\n",
        "    groups = re.findall(r'([A-Za-z0-9_]+)\\s*-\\s*\\(\\s*([0-9,\\s]+)\\s*\\)', inside)\n",
        "    results = []\n",
        "    for alias, nums in groups:\n",
        "        pages = [int(x) for x in re.findall(r'\\d+', nums)]\n",
        "        for p in pages:\n",
        "            results.append((alias.strip(), p))\n",
        "    return results[:5]\n",
        "\n",
        "\n",
        "def metrics_at_k(retrieved_pairs: list[tuple[str,int]], gt_alias: str, gt_pages: set[int], k: int = 5):\n",
        "\n",
        "    topk = retrieved_pairs[:k]\n",
        "    rel = []\n",
        "    for alias, page in topk:\n",
        "        rel.append(1 if (alias == gt_alias and page in gt_pages) else 0)\n",
        "\n",
        "    # Precision@k\n",
        "    prec = sum(rel) / max(k, 1)\n",
        "\n",
        "    # Recall@k (denominator = total number of relevant pages)\n",
        "    denom = max(len(gt_pages), 1)\n",
        "    rec = sum(rel) / denom\n",
        "\n",
        "    # F1@k\n",
        "    f1 = 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
        "\n",
        "    # Average Precision (AP)\n",
        "    running_hits = 0\n",
        "    ap_sum = 0.0\n",
        "    for i, r in enumerate(rel, start=1):\n",
        "        if r == 1:\n",
        "            running_hits += 1\n",
        "            ap_sum += running_hits / i\n",
        "    ap = ap_sum / denom  # standard AP normalization by total relevant\n",
        "\n",
        "    # Reciprocal Rank (RR)\n",
        "    rr = 0.0\n",
        "    for i, r in enumerate(rel, start=1):\n",
        "        if r == 1:\n",
        "            rr = 1.0 / i\n",
        "            break\n",
        "\n",
        "    return dict(Prec5=prec, Recall5=rec, F15=f1, AvgPrec=ap, RR=rr)\n",
        "\n",
        "\n",
        "EXCEL_PATH = \"Vector Discovery Benchmark 75.xlsx\"\n",
        "# Read the Excel file into a DataFrame\n",
        "df = pd.read_excel(EXCEL_PATH, sheet_name=\"Sheet1\")\n",
        "COL_TEXTBOOK = \"Textbook Name\"\n",
        "COL_GT = \"Ground Truth: [] means all pages in sequence\"\n",
        "COL_COS = \"Cosine Distance top 5\"\n",
        "COL_DOT = \"Dot product top 5\"\n",
        "COL_EUC = \"Euclidean distance top 5\"\n",
        "COL_MAN = \"Manhattan distance top 5\"\n",
        "\n",
        "df_filtered = df[df[\"Query ID\"].str.startswith(\"CP-\")]\n",
        "\n",
        "rows = []\n",
        "for _, row in df_filtered.iterrows():\n",
        "    gt_alias = extract_alias_from_textbook_name(row.get(COL_TEXTBOOK, \"\"))\n",
        "    gt_ranges = parse_ground_truth_ranges(row.get(COL_GT, \"\"))\n",
        "    gt_pages = range_to_pages(gt_ranges)\n",
        "\n",
        "    cos_pairs = parse_top5_string(str(row.get(COL_COS, \"\")))\n",
        "    dot_pairs = parse_top5_string(str(row.get(COL_DOT, \"\")))\n",
        "    euc_pairs = parse_top5_string(str(row.get(COL_EUC, \"\")))\n",
        "    man_pairs = parse_top5_string(str(row.get(COL_MAN, \"\")))\n",
        "\n",
        "    cos_m = metrics_at_k(cos_pairs, gt_alias, gt_pages, k=5)\n",
        "    dot_m = metrics_at_k(dot_pairs, gt_alias, gt_pages, k=5)\n",
        "    euc_m = metrics_at_k(euc_pairs, gt_alias, gt_pages, k=5)\n",
        "    man_m = metrics_at_k(man_pairs, gt_alias, gt_pages, k=5)\n",
        "\n",
        "    rows.append({\n",
        "        \"Query ID\": row.get(\"Query ID\"),\n",
        "        \"Cosine_Prec@5\": cos_m[\"Prec5\"],   \"Cosine_Recall@5\": cos_m[\"Recall5\"], \"Cosine_F1@5\": cos_m[\"F15\"],\n",
        "        \"Cosine_AvgPrec\": cos_m[\"AvgPrec\"], \"Cosine_RR\": cos_m[\"RR\"],\n",
        "\n",
        "        \"Dot_Prec@5\": dot_m[\"Prec5\"],      \"Dot_Recall@5\": dot_m[\"Recall5\"],    \"Dot_F1@5\": dot_m[\"F15\"],\n",
        "        \"Dot_AvgPrec\": dot_m[\"AvgPrec\"],   \"Dot_RR\": dot_m[\"RR\"],\n",
        "\n",
        "        \"Euclidean_Prec@5\": euc_m[\"Prec5\"],   \"Euclidean_Recall@5\": euc_m[\"Recall5\"], \"Euclidean_F1@5\": euc_m[\"F15\"],\n",
        "        \"Euclidean_AvgPrec\": euc_m[\"AvgPrec\"], \"Euclidean_RR\": euc_m[\"RR\"],\n",
        "\n",
        "        \"Manhattan_Prec@5\": man_m[\"Prec5\"],   \"Manhattan_Recall@5\": man_m[\"Recall5\"], \"Manhattan_F1@5\": man_m[\"F15\"],\n",
        "        \"Manhattan_AvgPrec\": man_m[\"AvgPrec\"], \"Manhattan_RR\": man_m[\"RR\"],\n",
        "    })\n",
        "\n",
        "per_query = pd.DataFrame(rows)\n",
        "\n",
        "avg_row = {\"Query ID\": \"AVERAGE\"}\n",
        "for col in per_query.columns:\n",
        "    if col == \"Query ID\":\n",
        "        continue\n",
        "    avg_row[col] = per_query[col].mean()\n",
        "\n",
        "# Append the average row to the DataFrame\n",
        "summary = pd.concat([per_query, pd.DataFrame([avg_row])], ignore_index=True)\n",
        "\n",
        "# Create a final summary table\n",
        "final_summary_conceptual = pd.DataFrame({\n",
        "    \"Metric\": [\"Prec@5\", \"Recall@5\", \"F1@5\", \"Avg Prec\", \"Recip Rank\"],\n",
        "    \"Cosine\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Cosine_RR\"].item(),\n",
        "    ],\n",
        "    \"Dot\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Dot_RR\"].item(),\n",
        "    ],\n",
        "    \"Euclidean\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Euclidean_RR\"].item(),\n",
        "    ],\n",
        "    \"Manh\": [\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_Prec@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_Recall@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_F1@5\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_AvgPrec\"].item(),\n",
        "        summary.loc[summary[\"Query ID\"]==\"AVERAGE\",\"Manhattan_RR\"].item(),\n",
        "    ],\n",
        "})\n",
        "\n",
        "# Print the final values\n",
        "print(final_summary_conceptual)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5oU_gCa0tjF",
        "outputId": "c47cca8e-0a01-46ac-eb2c-d77b947b39c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Metric    Cosine       Dot  Euclidean      Manh\n",
            "0      Prec@5  0.461538  0.400000   0.184615  0.323077\n",
            "1    Recall@5  0.226597  0.198625   0.087891  0.157056\n",
            "2        F1@5  0.303449  0.264988   0.118895  0.210958\n",
            "3    Avg Prec  0.182949  0.164806   0.059937  0.126731\n",
            "4  Recip Rank  0.692308  0.653846   0.358974  0.673077\n"
          ]
        }
      ]
    }
  ]
}